{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from adapters import BnConfig, AutoAdapterModel\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 60892\n",
      "Validation set size: 3383\n",
      "Test set size: 3383\n"
     ]
    }
   ],
   "source": [
    "# loading the english and maithili texts files\n",
    "with open(\"./dataset/train/bpcc/train.eng_Latn\", \"r\", encoding=\"utf-8\") as en_file:\n",
    "    eng_texts = en_file.readlines()\n",
    "\n",
    "with open(\"./dataset/train/bpcc/train.mai_Deva\", \"r\", encoding=\"utf-8\") as maithili_file:\n",
    "    mai_texts = maithili_file.readlines()\n",
    "\n",
    "assert len(eng_texts) == len(mai_texts), \"The number of sentences in both files must be the same.\"\n",
    "\n",
    "# cleaning the text files\n",
    "eng_texts_c = [text.strip() for text in eng_texts]\n",
    "mai_texts_c = [text.strip() for text in mai_texts]\n",
    "\n",
    "# creating the dataset\n",
    "data = {\n",
    "    \"source_text\": eng_texts_c,\n",
    "    \"target_text\": mai_texts_c\n",
    "}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# creating the train, val, test set\n",
    "train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1).values()\n",
    "val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.5).values()\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865ec4c47579496d9bcbe059f60f704a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60892 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786e39b072d54053bcf2785000d7fff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4febfac2e1e429bbf22164ad15f846d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16598b9aefc4ab798edaddeeb0e5eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/61 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d457a96fb8a340eb9c51a6195862120a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c1e805c6314756b8aa95b30cd42796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7350853"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "\n",
    "# defining the preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"source_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    targets = tokenizer(examples[\"target_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# applying preprocessing function to each split\n",
    "train_dataset_tokenized = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset_tokenized = val_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset_tokenized = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# saving dataset to csv (backup)\n",
    "train_dataset_tokenized.to_csv(\"./dataset/training/bpcc/train_dataset.csv\")\n",
    "val_dataset_tokenized.to_csv(\"./dataset/training/bpcc/val_dataset.csv\")\n",
    "test_dataset_tokenized.to_csv(\"./dataset/training/bpcc/test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moving the dataset to MPS (for training purposes)\n",
    "def move_to_device(batch):\n",
    "    # move each tensor in the batch to the MPS device\n",
    "    for key in batch:\n",
    "        batch[key] = torch.tensor(batch[key]).to(device)\n",
    "    return batch\n",
    "\n",
    "train_dataset_tokenized = train_dataset_tokenized.with_transform(move_to_device)\n",
    "val_dataset_tokenized = val_dataset_tokenized.with_transform(move_to_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding LoRA, Parameter Size: 76381184\n",
      "After adding LoRA, Parameter Size: 76676096\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Before adding LoRA, Parameter Size: {model.num_parameters()}\")\n",
    "\n",
    "# defining the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # The rank of the low-rank adaptation\n",
    "    lora_alpha=16,  # Scaling factor for the LoRA layers\n",
    "    lora_dropout=0.1,  # Dropout for the LoRA layers\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    bias=\"none\",  # You can set bias as 'none', 'all', or 'lora_only'\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Specify the target modules\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(f\"After adding LoRA, Parameter Size: {model.num_parameters()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training arguments and trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",  # Where to save results\n",
    "    eval_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    learning_rate=5e-5,  # Learning rate for fine-tuning\n",
    "    per_device_train_batch_size=16,  # Batch size (adjust based on GPU memory)\n",
    "    # gradient_accumulation_steps=2, # backpropagate every 2 steps\n",
    "    num_train_epochs=1,  # Number of training epochs\n",
    "    save_steps=1000,  # Save checkpoints after this many steps\n",
    "    logging_dir=\"./logs\",  # Directory for logs\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,  # Limit number of saved checkpoints\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,  # LoRA fine-tuned model\n",
    "    args=training_args,  # Training parameters\n",
    "    train_dataset=train_dataset_tokenized,  # Tokenized training dataset\n",
    "    eval_dataset=val_dataset_tokenized,\n",
    "    tokenizer=tokenizer  # Tokenizer for handling tokenization during training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0882ac3a324d1281abc9cd2cc43034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1691 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3508, 'grad_norm': 2.3712217807769775, 'learning_rate': 4.704316972205796e-05, 'epoch': 0.06}\n",
      "{'loss': 3.9726, 'grad_norm': 1.2145708799362183, 'learning_rate': 4.408633944411591e-05, 'epoch': 0.12}\n",
      "{'loss': 2.9302, 'grad_norm': 0.5059202313423157, 'learning_rate': 4.112950916617386e-05, 'epoch': 0.18}\n",
      "{'loss': 2.7166, 'grad_norm': 0.5383468270301819, 'learning_rate': 3.817267888823182e-05, 'epoch': 0.24}\n",
      "{'loss': 2.599, 'grad_norm': 0.3628119230270386, 'learning_rate': 3.521584861028977e-05, 'epoch': 0.3}\n",
      "{'loss': 2.5404, 'grad_norm': 0.3662428855895996, 'learning_rate': 3.225901833234772e-05, 'epoch': 0.35}\n",
      "{'loss': 2.52, 'grad_norm': 0.34807419776916504, 'learning_rate': 2.9302188054405678e-05, 'epoch': 0.41}\n",
      "{'loss': 2.4881, 'grad_norm': 0.35338684916496277, 'learning_rate': 2.634535777646363e-05, 'epoch': 0.47}\n",
      "{'loss': 2.4844, 'grad_norm': 0.3414859473705292, 'learning_rate': 2.3388527498521585e-05, 'epoch': 0.53}\n",
      "{'loss': 2.4701, 'grad_norm': 0.4289063811302185, 'learning_rate': 2.043169722057954e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/work/lib/python3.9/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 628a5f9a-48ff-47a0-8410-599c8eb12b75)') - silently ignoring the lookup for the file config.json in Helsinki-NLP/opus-mt-en-hi.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/work/lib/python3.9/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in Helsinki-NLP/opus-mt-en-hi - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4595, 'grad_norm': 0.5130706429481506, 'learning_rate': 1.7474866942637493e-05, 'epoch': 0.65}\n",
      "{'loss': 2.4485, 'grad_norm': 0.32000532746315, 'learning_rate': 1.4518036664695447e-05, 'epoch': 0.71}\n",
      "{'loss': 2.4441, 'grad_norm': 0.30320510268211365, 'learning_rate': 1.15612063867534e-05, 'epoch': 0.77}\n",
      "{'loss': 2.424, 'grad_norm': 0.323080837726593, 'learning_rate': 8.604376108811355e-06, 'epoch': 0.83}\n",
      "{'loss': 2.4135, 'grad_norm': 0.47815823554992676, 'learning_rate': 5.647545830869308e-06, 'epoch': 0.89}\n",
      "{'loss': 2.4244, 'grad_norm': 0.3734165132045746, 'learning_rate': 2.6907155529272622e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78c6a53227d4fff8de2078df7bbc44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3473432064056396, 'eval_runtime': 158.4219, 'eval_samples_per_second': 42.709, 'eval_steps_per_second': 5.34, 'epoch': 1.0}\n",
      "{'train_runtime': 4269.1797, 'train_samples_per_second': 12.678, 'train_steps_per_second': 0.396, 'train_loss': 2.8314020121894172, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_model/tokenizer_config.json',\n",
       " './fine_tuned_model/special_tokens_map.json',\n",
       " './fine_tuned_model/vocab.json',\n",
       " './fine_tuned_model/source.spm',\n",
       " './fine_tuned_model/target.spm',\n",
       " './fine_tuned_model/added_tokens.json')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./finetuned_epoch\")\n",
    "tokenizer.save_pretrained(\"./finetuned_epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "प्राक प्राक सार्रा सारारा सारारारा स्रारा स्रारारा स्रारारा स्रारारारारारारा क स्रारारारारारारारारारारारा स स्रारारारा क स्रा\n"
     ]
    }
   ],
   "source": [
    "# Example input text\n",
    "input_text = \"Penicillin is an effective treatment for syphilis in pregnancy but there is no agreement on which dose or route of delivery is most effective\"\n",
    "\n",
    "# Tokenize the input text\n",
    "encoded_input = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "\n",
    "# Generate prediction\n",
    "output_sequences = model.generate(\n",
    "    input_ids=encoded_input[\"input_ids\"],\n",
    "    attention_mask=encoded_input[\"attention_mask\"],\n",
    "    max_length=128,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode the generated sequences\n",
    "predicted_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
