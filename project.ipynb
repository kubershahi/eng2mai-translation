{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: dill in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/kuber/.local/lib/python3.9/site-packages (from evaluate) (4.66.6)\n",
      "Requirement already satisfied: xxhash in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from evaluate) (0.26.3)\n",
      "Requirement already satisfied: packaging in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from evaluate) (24.1)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: regex in /Users/kuber/.local/lib/python3.9/site-packages (from sacrebleu) (2024.9.11)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: colorama in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from sacrebleu) (0.4.6)\n",
      "Collecting lxml (from sacrebleu)\n",
      "  Downloading lxml-5.3.0-cp39-cp39-macosx_10_9_universal2.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.11.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from pandas->evaluate) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/work/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading lxml-5.3.0-cp39-cp39-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: tabulate, portalocker, lxml, sacrebleu, evaluate\n",
      "Successfully installed evaluate-0.4.3 lxml-5.3.0 portalocker-3.0.0 sacrebleu-2.4.3 tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/work/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/miniconda3/envs/work/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <843938F4-8FEE-3058-B0A3-50B73FAF02AB> /opt/miniconda3/envs/work/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/miniconda3/envs/work/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/miniconda3/envs/work/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/miniconda3/envs/work/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/miniconda3/envs/work/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "import evaluate\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1) Preparing and tokenizing the training datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prepare the training and any other dataset\n",
    "def prepare_data(eng_file_path, mai_file_path, data_type):\n",
    "    # load the english and maithili texts files\n",
    "    with open(eng_file_path, \"r\", encoding=\"utf-8\") as en_file:\n",
    "        eng_texts = en_file.readlines()\n",
    "\n",
    "    with open(mai_file_path, \"r\", encoding=\"utf-8\") as maithili_file:\n",
    "        mai_texts = maithili_file.readlines()\n",
    "\n",
    "    assert len(eng_texts) == len(mai_texts), \"The number of sentences in both files must be the same.\"\n",
    "\n",
    "    # clean the text files\n",
    "    eng_texts_cleaned = [text.strip() for text in eng_texts]\n",
    "    mai_texts_cleaned = [text.strip() for text in mai_texts]\n",
    "\n",
    "    # create the dataset\n",
    "    data = {\n",
    "        \"source_text\": eng_texts_cleaned,\n",
    "        \"target_text\": mai_texts_cleaned, \n",
    "    }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "\n",
    "    # split the dataset into train, validation and test sets\n",
    "    if data_type == \"train\":\n",
    "        train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1).values()\n",
    "        val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.5).values()\n",
    "\n",
    "        print(f\"Training set size: {len(train_dataset)}\")\n",
    "        print(f\"Validation set size: {len(val_dataset)}\")\n",
    "        print(f\"Test set size: {len(test_dataset)}\")\n",
    "\n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "    elif data_type == \"test\":\n",
    "        return dataset\n",
    "\n",
    "# preprocessor function for tokenizer\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    inputs = tokenizer(examples[\"source_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    targets = tokenizer(examples[\"target_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# function to tokenize the data\n",
    "def tokenize_data(dataset, tokenizer):\n",
    "    dataset_tokenized = dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n",
    "    return dataset_tokenized\n",
    "\n",
    "# function to move the dataset to device\n",
    "def move_to_device(batch):\n",
    "    # move each tensor in the batch to the MPS device\n",
    "    for key in batch:\n",
    "        batch[key] = torch.tensor(batch[key]).to(device)\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 60892\n",
      "Validation set size: 3383\n",
      "Test set size: 3383\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4298ebfcde504fb49ca10eb1689c02fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60892 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f268c7cc9cc44196af17439aaecf6c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b29cd0214a24313a27d43a298188d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare the data\n",
    "train_dataset, val_dataset, test_dataset = prepare_data(\"./dataset/train/bpcc/train.eng_Latn\", \"./dataset/train/bpcc/train.mai_Deva\", \"train\")\n",
    "\n",
    "# tokenize the data\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\") # load the tokenizer\n",
    "train_dataset_tokenized = tokenize_data(train_dataset, tokenizer)\n",
    "val_dataset_tokenized = tokenize_data(val_dataset, tokenizer)\n",
    "test_dataset_tokenized = tokenize_data(test_dataset, tokenizer)\n",
    "\n",
    "# # saving dataset to csv (backup)\n",
    "# train_dataset_tokenized.to_csv(\"./dataset/training/bpcc/train_dataset.csv\")\n",
    "# val_dataset_tokenized.to_csv(\"./dataset/training/bpcc/val_dataset.csv\")\n",
    "# test_dataset_tokenized.to_csv(\"./dataset/training/bpcc/test_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source text:  Although he admired Godard's \"revolutionary\" early phase, he thought his later phase was \"alien\".\n",
      "Target text:  यद्यपि ओ गोडार्डक \"क्रान्तिकारी\" प्रारम्भिक चरणक प्रशंसा कयलनि, मुदा हुनक सोच छल जे हुनक बादक चरण \"बेगाना\" छल।\n",
      "Source tokens:  ['▁Although', '▁he', '▁admired', '▁God', 'ard', \"'\", 's', '▁\"', 'r', 'evolutionary', '\"', '▁early', '▁phase', ',', '▁he', '▁thought', '▁his', '▁later', '▁phase', '▁was', '▁\"', 'alien', '\".', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Target tokens:  ['▁', '<unk>', '्', 'य', 'प', 'ि', '▁', 'ओ', '▁', 'ग', 'ो', 'ड', 'ा', 'र', '्', '<unk>', '▁\"', 'क', '्', 'र', 'ा', 'न', '्', 'तिक', 'ा', 'र', 'ी', '\"', '▁', 'प', '्', 'र', 'ा', 'र', 'म', '्', '<unk>', '▁', 'च', 'र', '<unk>', '▁', 'प', '्', 'र', 'श', 'ं', 'स', 'ा', '▁', '<unk>', 'ल', 'न', 'ि', ',', '▁', 'म', '<unk>', 'ा', '▁', 'हु', 'न', 'क', '▁', 'स', 'ो', 'च', '▁', 'छ', 'ल', '▁', 'जे', '▁', 'हु', 'न', 'क', '▁', 'ब', 'ा', '<unk>', '▁', 'च', 'र', 'ण', '▁\"', 'ब', 'े', 'ग', 'ा', 'न', 'ा', '\"', '▁', 'छ', 'ल', '।', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# checking the tokenization and vocab subwords\n",
    "print(\"Source text: \", train_dataset_tokenized[0][\"source_text\"])\n",
    "print(\"Target text: \", train_dataset_tokenized[0][\"target_text\"])\n",
    "print(\"Source tokens: \", tokenizer.convert_ids_to_tokens(train_dataset_tokenized[0][\"input_ids\"]))\n",
    "print(\"Target tokens: \", tokenizer.convert_ids_to_tokens(train_dataset_tokenized[0][\"labels\"]))\n",
    "\n",
    "# save the tokenized output to a text file\n",
    "with open(\"dataset/training/tokenized_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(train_dataset_tokenized[0][\"source_text\"] + \"\\n\")\n",
    "    f.write(train_dataset_tokenized[0][\"target_text\"] + \"\\n\")\n",
    "    f.write(\" \".join(tokenizer.convert_ids_to_tokens(train_dataset_tokenized[0][\"input_ids\"])) + \"\\n\")\n",
    "    f.write(\" \".join(tokenizer.convert_ids_to_tokens(train_dataset_tokenized[0][\"labels\"])) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the training and validation dataset to the MPS device\n",
    "train_dataset_tokenized = train_dataset_tokenized.with_transform(move_to_device)\n",
    "val_dataset_tokenized = val_dataset_tokenized.with_transform(move_to_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2) Loading the pretrained model and testing its performance on benchmark datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Text:  An appearance is a bunch of attributes related to the service person, like their shoes, clothes, tie, jewellery, hairstyle, make-up, watch, cosmetics, perfume, etc.\n",
      "Hindi Text:  सेवा संबंधी लोगों के लिए भेष कई गुणों का संयोजन है, जैसे कि उनके जूते, कपड़े, टाई, आभूषण, केश शैली, मेक-अप, घड़ी, कॉस्मेटिक, इत्र, आदि।\n",
      "Maithili Text:  रूप सर्विसवला व्यक्तिसँ सम्बन्धित बहुत रास लक्षणक समूह होयत छै जेना हुनक जूता, कपड़ा, टाई, गहना, केश, श्रृंगार, घड़ी, प्रसाधन सामग्री, सेंट इत्यादि।\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae31db4e416a4395bd5fe91327586988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586fd49fb715442f81802a12b6ca60dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the model to device\n",
    "model_name = \"Helsinki-NLP/opus-mt-en-hi\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "# prepare the benchmark datasets\n",
    "in22_hin_test = prepare_data(\"./dataset/test/IN22_test/gen/test.eng_Latn\", \"./dataset/test/IN22_test/gen/test.hin_Deva\", \"test\")\n",
    "in22_mai_test = prepare_data(\"./dataset/test/IN22_test/gen/test.eng_Latn\", \"./dataset//test/IN22_test/gen/test.mai_Deva\", \"test\")\n",
    "\n",
    "print(\"English Text: \", in22_hin_test[0][\"source_text\"])\n",
    "print(\"Hindi Text: \", in22_hin_test[0][\"target_text\"])\n",
    "print(\"Maithili Text: \", in22_mai_test[0][\"target_text\"])\n",
    "\n",
    "# tokenize the benchmark datasets\n",
    "in22_hin_test_tokenized = tokenize_data(in22_hin_test, tokenizer)\n",
    "in22_mai_test_tokenized = tokenize_data(in22_mai_test, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the model on the benchmark datasets\n",
    "\n",
    "# generate predictions for english to hindi\n",
    "def generate_predictions(model, tokenizer, test_dataset):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    model.eval()  # set model to evaluation mode\n",
    "    for example in tqdm(test_dataset, desc=\"Generating predictions: \", unit=\"example\"):\n",
    "        inputs = torch.tensor(example['input_ids']).unsqueeze(0).to(model.device)  # move input to device\n",
    "        attention_mask = torch.tensor(example['attention_mask']).unsqueeze(0).to(model.device) # move attention mask to device\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.generate(inputs, attention_mask=attention_mask, max_length=128) # generate the output\n",
    "        \n",
    "        prediction = tokenizer.decode(output[0], skip_special_tokens=True) # decode the output\n",
    "        target = example['target_text']  # if the target is already a string\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        references.append(target)\n",
    "    \n",
    "    return predictions, references\n",
    "\n",
    "hin_predictions, hin_references = generate_predictions(model, tokenizer, in22_hin_test_tokenized)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hin_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhin_predictions\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hin_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "hin_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e294a1202b4c9babc56e8fa0344e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/9.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'hin_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m chrf \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchrf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# calculate chrF++ score for english to hindi translation\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m chrf_score_hin \u001b[38;5;241m=\u001b[39m chrf\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39m\u001b[43mhin_predictions\u001b[49m, references\u001b[38;5;241m=\u001b[39mhin_references)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchrF++ score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchrf_score_hin[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# calclate the chrF++ score for hindi to maithili overlap\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hin_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the chrF++ metric from the evaluate library\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "\n",
    "# calculate chrF++ score for english to hindi translation\n",
    "chrf_score_hin = chrf.compute(predictions=predictions, references=references)\n",
    "print(f\"chrF++ score: {chrf_score_hin['score']}\")\n",
    "\n",
    "# calclate the chrF++ score for hindi to maithili overlap\n",
    "mai_references = [example[\"target_text\"] for example in in22_mai_test]\n",
    "chrF_score_mai = chrf.compute(predictions=hin_predictions, references=mai_references)\n",
    "print(f\"chrF++ score: {chrF_score_mai['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before adding LoRA, Parameter Size: 76381184\n",
      "After adding LoRA, Parameter Size: 76676096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Before adding LoRA, Parameter Size: {model.num_parameters()}\")\n",
    "\n",
    "# defining the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # The rank of the low-rank adaptation\n",
    "    lora_alpha=16,  # Scaling factor for the LoRA layers\n",
    "    lora_dropout=0.1,  # Dropout for the LoRA layers\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    bias=\"none\",  # You can set bias as 'none', 'all', or 'lora_only'\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Specify the target modules\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(f\"After adding LoRA, Parameter Size: {model.num_parameters()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup training arguments and trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",  # Where to save results\n",
    "    eval_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
    "    learning_rate=5e-5,  # Learning rate for fine-tuning\n",
    "    per_device_train_batch_size=16,  # Batch size (adjust based on GPU memory)\n",
    "    # gradient_accumulation_steps=2, # backpropagate every 2 steps\n",
    "    num_train_epochs=1,  # Number of training epochs\n",
    "    save_steps=1000,  # Save checkpoints after this many steps\n",
    "    logging_dir=\"./logs\",  # Directory for logs\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,  # Limit number of saved checkpoints\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,  # LoRA fine-tuned model\n",
    "    args=training_args,  # Training parameters\n",
    "    train_dataset=train_dataset_tokenized,  # Tokenized training dataset\n",
    "    eval_dataset=val_dataset_tokenized,\n",
    "    tokenizer=tokenizer  # Tokenizer for handling tokenization during training\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0882ac3a324d1281abc9cd2cc43034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1691 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3508, 'grad_norm': 2.3712217807769775, 'learning_rate': 4.704316972205796e-05, 'epoch': 0.06}\n",
      "{'loss': 3.9726, 'grad_norm': 1.2145708799362183, 'learning_rate': 4.408633944411591e-05, 'epoch': 0.12}\n",
      "{'loss': 2.9302, 'grad_norm': 0.5059202313423157, 'learning_rate': 4.112950916617386e-05, 'epoch': 0.18}\n",
      "{'loss': 2.7166, 'grad_norm': 0.5383468270301819, 'learning_rate': 3.817267888823182e-05, 'epoch': 0.24}\n",
      "{'loss': 2.599, 'grad_norm': 0.3628119230270386, 'learning_rate': 3.521584861028977e-05, 'epoch': 0.3}\n",
      "{'loss': 2.5404, 'grad_norm': 0.3662428855895996, 'learning_rate': 3.225901833234772e-05, 'epoch': 0.35}\n",
      "{'loss': 2.52, 'grad_norm': 0.34807419776916504, 'learning_rate': 2.9302188054405678e-05, 'epoch': 0.41}\n",
      "{'loss': 2.4881, 'grad_norm': 0.35338684916496277, 'learning_rate': 2.634535777646363e-05, 'epoch': 0.47}\n",
      "{'loss': 2.4844, 'grad_norm': 0.3414859473705292, 'learning_rate': 2.3388527498521585e-05, 'epoch': 0.53}\n",
      "{'loss': 2.4701, 'grad_norm': 0.4289063811302185, 'learning_rate': 2.043169722057954e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/work/lib/python3.9/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 628a5f9a-48ff-47a0-8410-599c8eb12b75)') - silently ignoring the lookup for the file config.json in Helsinki-NLP/opus-mt-en-hi.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/work/lib/python3.9/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in Helsinki-NLP/opus-mt-en-hi - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4595, 'grad_norm': 0.5130706429481506, 'learning_rate': 1.7474866942637493e-05, 'epoch': 0.65}\n",
      "{'loss': 2.4485, 'grad_norm': 0.32000532746315, 'learning_rate': 1.4518036664695447e-05, 'epoch': 0.71}\n",
      "{'loss': 2.4441, 'grad_norm': 0.30320510268211365, 'learning_rate': 1.15612063867534e-05, 'epoch': 0.77}\n",
      "{'loss': 2.424, 'grad_norm': 0.323080837726593, 'learning_rate': 8.604376108811355e-06, 'epoch': 0.83}\n",
      "{'loss': 2.4135, 'grad_norm': 0.47815823554992676, 'learning_rate': 5.647545830869308e-06, 'epoch': 0.89}\n",
      "{'loss': 2.4244, 'grad_norm': 0.3734165132045746, 'learning_rate': 2.6907155529272622e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78c6a53227d4fff8de2078df7bbc44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3473432064056396, 'eval_runtime': 158.4219, 'eval_samples_per_second': 42.709, 'eval_steps_per_second': 5.34, 'epoch': 1.0}\n",
      "{'train_runtime': 4269.1797, 'train_samples_per_second': 12.678, 'train_steps_per_second': 0.396, 'train_loss': 2.8314020121894172, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_model/tokenizer_config.json',\n",
       " './fine_tuned_model/special_tokens_map.json',\n",
       " './fine_tuned_model/vocab.json',\n",
       " './fine_tuned_model/source.spm',\n",
       " './fine_tuned_model/target.spm',\n",
       " './fine_tuned_model/added_tokens.json')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./finetuned_epoch\")\n",
    "tokenizer.save_pretrained(\"./finetuned_epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "प्राक प्राक सार्रा सारारा सारारारा स्रारा स्रारारा स्रारारा स्रारारारारारारा क स्रारारारारारारारारारारारा स स्रारारारा क स्रा\n"
     ]
    }
   ],
   "source": [
    "# Example input text\n",
    "input_text = \"Penicillin is an effective treatment for syphilis in pregnancy but there is no agreement on which dose or route of delivery is most effective\"\n",
    "\n",
    "# Tokenize the input text\n",
    "encoded_input = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "\n",
    "# Generate prediction\n",
    "output_sequences = model.generate(\n",
    "    input_ids=encoded_input[\"input_ids\"],\n",
    "    attention_mask=encoded_input[\"attention_mask\"],\n",
    "    max_length=128,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode the generated sequences\n",
    "predicted_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
