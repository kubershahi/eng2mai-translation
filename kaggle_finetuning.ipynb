{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10092252,"sourceType":"datasetVersion","datasetId":6223367},{"sourceId":10092326,"sourceType":"datasetVersion","datasetId":6223428}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate sacremoses sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:49:41.511818Z","iopub.execute_input":"2024-12-04T03:49:41.512184Z","iopub.status.idle":"2024-12-04T03:49:50.027502Z","shell.execute_reply.started":"2024-12-04T03:49:41.512152Z","shell.execute_reply":"2024-12-04T03:49:50.026633Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.3)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.10/site-packages (0.1.1)\nRequirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (2.4.3)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacremoses) (2024.5.15)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses) (1.4.2)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (3.0.0)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport torch\nimport evaluate\nfrom tqdm import tqdm\n\nfrom datasets import Dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:49:53.276252Z","iopub.execute_input":"2024-12-04T03:49:53.276639Z","iopub.status.idle":"2024-12-04T03:49:53.281917Z","shell.execute_reply.started":"2024-12-04T03:49:53.276603Z","shell.execute_reply":"2024-12-04T03:49:53.280780Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# input_path = \"/kaggle/working\"\n\n# for root, dirs, files in os.walk(input_path):\n#     print(f\"Directory: {root}\")\n#     for file in files:\n#         print(f\"  File: {file}\")\n\n\n# file_path = \"./test.mai_Deva\"\n# if os.path.exists(file_path):\n#     os.remove(file_path)\n# else: \n#     print(\"file not found\")\n\n# dir_path = \"\"\n# # Remove the directory\n# try:\n#     os.rmdir(dir_path)\n#     print(f\"Directory {dir_path} has been removed.\")\n# except OSError as e:\n#     print(f\"Error: {e}\")\n\n# directory_path = \"./logs/\"\n# os.makedirs(directory_path, exist_ok=True)\n\n!ls /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:49:56.067788Z","iopub.execute_input":"2024-12-04T03:49:56.068146Z","iopub.status.idle":"2024-12-04T03:49:57.110384Z","shell.execute_reply.started":"2024-12-04T03:49:56.068113Z","shell.execute_reply":"2024-12-04T03:49:57.109478Z"}},"outputs":[{"name":"stdout","text":"finetuned  state.db  test_split  wandb\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### Part 1) Preparing and tokenizing the training datasets","metadata":{}},{"cell_type":"code","source":"# function to prepare the training and any other dataset\ndef prepare_data(eng_file_path, mai_file_path, data_type):\n    # load the english and maithili texts files\n    with open(eng_file_path, \"r\", encoding=\"utf-8\") as en_file:\n        eng_texts = en_file.readlines()\n\n    with open(mai_file_path, \"r\", encoding=\"utf-8\") as maithili_file:\n        mai_texts = maithili_file.readlines()\n\n    assert len(eng_texts) == len(mai_texts), \"The number of sentences in both files must be the same.\"\n\n    # clean the text files\n    eng_texts_cleaned = [text.strip() for text in eng_texts]\n    mai_texts_cleaned = [text.strip() for text in mai_texts]\n\n    # create the dataset\n    data = {\n        \"source_text\": eng_texts_cleaned,\n        \"target_text\": mai_texts_cleaned, \n    }\n    dataset = Dataset.from_dict(data)\n\n    # split the dataset into train, validation and test sets\n    if data_type == \"train\":\n        train_dataset, temp_dataset = dataset.train_test_split(test_size=0.1).values()\n        val_dataset, test_dataset = temp_dataset.train_test_split(test_size=0.5).values()\n\n        print(f\"Training set size: {len(train_dataset)}\")\n        print(f\"Validation set size: {len(val_dataset)}\")\n        print(f\"Test set size: {len(test_dataset)}\")\n\n        return train_dataset, val_dataset, test_dataset\n    elif data_type == \"test\":\n        return dataset\n\n# preprocessor function for tokenizer\ndef preprocess_function(examples, tokenizer):\n    inputs = tokenizer(examples[\"source_text\"], truncation=True, padding=\"max_length\", max_length=128)\n    targets = tokenizer(examples[\"target_text\"], truncation=True, padding=\"max_length\", max_length=128)\n    inputs[\"labels\"] = targets[\"input_ids\"]\n    return inputs\n\n# function to tokenize the data\ndef tokenize_dataset(dataset, tokenizer):\n    dataset_tokenized = dataset.map(lambda x: preprocess_function(x, tokenizer), batched=True)\n    return dataset_tokenized\n\ndef save_dataset(dataset, file_path):\n    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n        for line in dataset: \n            f.write(line + \"\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:50:24.706616Z","iopub.execute_input":"2024-12-04T03:50:24.706998Z","iopub.status.idle":"2024-12-04T03:50:24.716957Z","shell.execute_reply.started":"2024-12-04T03:50:24.706964Z","shell.execute_reply":"2024-12-04T03:50:24.716033Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# prepare the data\ntrain_dataset, val_dataset, test_dataset = prepare_data(\"/kaggle/input/bpcc-eng-mai-train/bpcc/train.eng_Latn\", \"/kaggle/input/bpcc-eng-mai-train/bpcc/train.mai_Deva\", \"train\")\n\n# tokenize the data\ntokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\") # load the tokenizer\ntrain_dataset_tokenized = tokenize_dataset(train_dataset, tokenizer)\nval_dataset_tokenized = tokenize_dataset(val_dataset, tokenizer)\ntest_dataset_tokenized = tokenize_dataset(test_dataset, tokenizer)\n\n# # saving dataset to csv (backup)\n# save_dataset(train_dataset[\"source_text\"], \"./dataset/training_split/bpcc/train.eng_Latn\")\n# save_dataset(train_dataset[\"target_text\"], \"./dataset/training_split/bpcc/train.mai_Deva\")\n# save_dataset(val_dataset[\"source_text\"], \"./dataset/training_split/bpcc/val.eng_Latn\")\n# save_dataset(val_dataset[\"target_text\"], \"./dataset/training_split/bpcc/val.mai_Deva\")\nsave_dataset(test_dataset[\"source_text\"], \"./test_split/test.eng_Latn\")\nsave_dataset(test_dataset[\"target_text\"], \"./test_split/test.mai_Deva\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:50:27.536975Z","iopub.execute_input":"2024-12-04T03:50:27.537853Z","iopub.status.idle":"2024-12-04T03:50:54.699846Z","shell.execute_reply.started":"2024-12-04T03:50:27.537801Z","shell.execute_reply":"2024-12-04T03:50:54.699127Z"}},"outputs":[{"name":"stdout","text":"Training set size: 60892\nValidation set size: 3383\nTest set size: 3383\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e17f1472dd7b45b6a3dfc2ed9ef4fa7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a409f1325a75477dbbef6abf1433e1e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/812k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7038261c4ea94b2da103a735f254cc17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/1.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22a3c28e36b94254a4b9bf38dc759ad5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d62bcaf587bb4a2e981ddc8f14ae996d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/60892 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2ce5b191cd948729a6a131d78f78dfc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3383 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23ccf76fecea490e9cdc7e65e01d6350"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3383 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e9d5bdb144341259e68701ff3d4f0d0"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# checking the tokenization and vocab subwords\nprint(\"Source text: \", train_dataset_tokenized[0][\"source_text\"])\nprint(\"Target text: \", train_dataset_tokenized[0][\"target_text\"])\nprint(\"Source tokens: \", tokenizer.convert_ids_to_tokens(train_dataset_tokenized[0][\"input_ids\"]))\nprint(\"Target tokens: \", tokenizer.convert_ids_to_tokens(train_dataset_tokenized[0][\"labels\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:50:55.949692Z","iopub.execute_input":"2024-12-04T03:50:55.950052Z","iopub.status.idle":"2024-12-04T03:50:55.959325Z","shell.execute_reply.started":"2024-12-04T03:50:55.950020Z","shell.execute_reply":"2024-12-04T03:50:55.958300Z"}},"outputs":[{"name":"stdout","text":"Source text:  Rail transport was absent in the state until 2008–09 when the railway track was extended to the capital Agartala.\nTarget text:  २००८-०९ धरि राज्यमे रेल परिवहन अनुपस्थित छल, जखन रेल पटरीकेँ राजधानी अगरतला धरि बढ़ाओल गेल छल।\nSource tokens:  ['▁Ra', 'il', '▁transport', '▁was', '▁absent', '▁in', '▁the', '▁state', '▁until', '▁2008', '–', '09', '▁when', '▁the', '▁railway', '▁track', '▁was', '▁extended', '▁to', '▁the', '▁capital', '▁A', 'gar', 't', 'ala', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\nTarget tokens:  ['▁', '<unk>', '-', '०९', '▁', 'ध', 'र', 'ि', '▁', 'र', 'ा', 'ज', '्', 'य', 'म', 'े', '▁', 'र', 'े', 'ल', '▁', 'प', 'र', '<unk>', 'न', '▁', 'अ', 'न', 'ु', 'प', 'स', '्', 'थित', '▁', 'छ', 'ल', ',', '▁', '<unk>', 'न', '▁', 'र', 'े', 'ल', '▁', 'प', 'ट', 'र', 'ी', '<unk>', '▁', 'र', 'ा', '<unk>', 'ा', 'न', 'ी', '▁', 'अ', 'ग', 'र', 'त', 'ल', 'ा', '▁', 'ध', 'र', 'ि', '▁', 'ब', 'ढ', '़', 'ा', 'ओ', 'ल', '▁', 'ग', 'े', 'ल', '▁', 'छ', 'ल', '।', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"### Part 2) Fine tuning the pretrained model","metadata":{}},{"cell_type":"code","source":"# funtion to configure LoRA\ndef configure_lora(rank, lora_alpha, lora_dropout, task_type, bias, target_modules):\n\n    lora_config = LoraConfig(\n        r=rank,  # rank of the low-rank adaptation\n        lora_alpha=lora_alpha,  # scaling factor for the LoRA layers\n        lora_dropout=lora_dropout,  # dropout for the LoRA layers\n        task_type=task_type, # task type\n        bias=bias,  # set bias as 'none', 'all', or 'lora_only'\n        target_modules=target_modules  # specify the target modules\n    )\n\n    return lora_config\n\n# function to define training arguments\ndef train_args(output_dir, eval_stra, learning_r, batch_size, grad_step, num_train_epochs, \n            save_steps, logging_dir, logging_steps, save_total_limit):\n\n    training_args = Seq2SeqTrainingArguments(\n        output_dir=output_dir,  # directory to save results\n        eval_strategy=eval_stra,  # strategy for evaluation\n        learning_rate=learning_r,  # learnin rate for fine-tuning\n        per_device_train_batch_size=batch_size,  # batch size\n        gradient_accumulation_steps= grad_step, # step for backpropagation\n        num_train_epochs=num_train_epochs,  # number of training epochs\n        save_steps=save_steps,  # save checkpoints after this many steps\n        logging_dir=logging_dir,  # directory for logs\n        logging_steps=logging_steps, # log after this many steps\n        save_total_limit=save_total_limit,  # limit number of saved checkpoints\n    )\n\n    return training_args\n\n# function to define the training trainer\ndef trainer_train(model, training_args, train_dataset, val_dataset, tokenizer):\n\n    trainer = Seq2SeqTrainer(\n        model=model,  # model to be fine-tuned\n        args=training_args,  # training arguments\n        train_dataset=train_dataset,  # tokenized training dataset\n        eval_dataset=val_dataset, # tokenized validation dataset\n        tokenizer=tokenizer  # tokenizer for tokenization\n    )\n\n    return trainer\n\n# function to print the trainable parameters\ndef get_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"After adding LoRA: Trainable Params: {trainable_params}, All Params: {all_param},  Trainable %: {100 * trainable_params / all_param:.2f}\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:50:59.484468Z","iopub.execute_input":"2024-12-04T03:50:59.484884Z","iopub.status.idle":"2024-12-04T03:50:59.492550Z","shell.execute_reply.started":"2024-12-04T03:50:59.484837Z","shell.execute_reply":"2024-12-04T03:50:59.491537Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# load the model\nmodel_name = \"Helsinki-NLP/opus-mt-en-hi\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\nprint(\"Parameter Size (pretrained): \", model.num_parameters())\nprint(\"Training Dataset length: \", len(train_dataset_tokenized))\nprint(\"Validation Dataset length: \", len(val_dataset_tokenized))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:51:03.340460Z","iopub.execute_input":"2024-12-04T03:51:03.340822Z","iopub.status.idle":"2024-12-04T03:51:06.643192Z","shell.execute_reply.started":"2024-12-04T03:51:03.340789Z","shell.execute_reply":"2024-12-04T03:51:06.642342Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/306M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c52a453549e4e4e932c5862a96b4c97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efe930d72a9348958cbc340a9a3704cf"}},"metadata":{}},{"name":"stdout","text":"Parameter Size (pretrained):  76381184\nTraining Dataset length:  60892\nValidation Dataset length:  3383\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# define training arguments and trainer\ntraining_args = train_args(output_dir=\"/results/epoch5\", eval_stra=\"epoch\", learning_r=5e-5, \n                           batch_size=32, grad_step=1, num_train_epochs=5, save_steps=1000,\n                           logging_dir=\"./logs/epoch5\", logging_steps=500, save_total_limit=2)\n\ntrain_trainer = trainer_train(model, training_args, train_dataset_tokenized, val_dataset_tokenized, tokenizer)\n\n# train the model\nprint(\"Finetuning the pretrained model\")\ntrain_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T03:52:01.367961Z","iopub.execute_input":"2024-12-04T03:52:01.368621Z","iopub.status.idle":"2024-12-04T05:03:16.774529Z","shell.execute_reply.started":"2024-12-04T03:52:01.368585Z","shell.execute_reply":"2024-12-04T05:03:16.773799Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_23/3856479509.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Finetuning the pretrained model\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241204_035221-d81chbq7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/kubershahi-uc-san-diego/huggingface/runs/d81chbq7' target=\"_blank\">/results/epoch5</a></strong> to <a href='https://wandb.ai/kubershahi-uc-san-diego/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/kubershahi-uc-san-diego/huggingface' target=\"_blank\">https://wandb.ai/kubershahi-uc-san-diego/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/kubershahi-uc-san-diego/huggingface/runs/d81chbq7' target=\"_blank\">https://wandb.ai/kubershahi-uc-san-diego/huggingface/runs/d81chbq7</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4760' max='4760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4760/4760 1:10:52, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.539400</td>\n      <td>0.956963</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.956900</td>\n      <td>0.773835</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.796100</td>\n      <td>0.689845</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.721500</td>\n      <td>0.651697</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.681300</td>\n      <td>0.639220</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[61949]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4760, training_loss=0.8909465180725611, metrics={'train_runtime': 4273.494, 'train_samples_per_second': 71.244, 'train_steps_per_second': 1.114, 'total_flos': 1.032069618597888e+16, 'train_loss': 0.8909465180725611, 'epoch': 5.0})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# save the fintuned model\nmodel.save_pretrained(\"./finetuned/epoch5\")\ntokenizer.save_pretrained(\"./finetuned/epoch5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T05:15:59.450506Z","iopub.execute_input":"2024-12-04T05:15:59.450855Z","iopub.status.idle":"2024-12-04T05:16:00.247580Z","shell.execute_reply.started":"2024-12-04T05:15:59.450824Z","shell.execute_reply":"2024-12-04T05:16:00.246766Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"('./finetuned/epoch5/tokenizer_config.json',\n './finetuned/epoch5/special_tokens_map.json',\n './finetuned/epoch5/vocab.json',\n './finetuned/epoch5/source.spm',\n './finetuned/epoch5/target.spm',\n './finetuned/epoch5/added_tokens.json')"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"### Part 3) Testing finetuned model's performance on test data split\n","metadata":{}},{"cell_type":"code","source":"# define trainer for evaluation\ndef trainer_evaluate(model, tokenizer, test_dataset):\n\n    eval_trainer = Seq2SeqTrainer(\n        model=model,\n        args = Seq2SeqTrainingArguments(\n            output_dir=\"./results/test/\",\n            per_device_eval_batch_size=32,\n            predict_with_generate=True,\n            disable_tqdm=False,\n        ), \n        eval_dataset=test_dataset,\n        tokenizer=tokenizer,\n    )\n\n    return eval_trainer\n\ndef compute_chrf(predictions, references):\n    chrf = evaluate.load(\"chrf\")\n    chrf_score = chrf.compute(predictions=predictions, references=references, word_order=2)\n    return chrf_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T05:16:16.534661Z","iopub.execute_input":"2024-12-04T05:16:16.534991Z","iopub.status.idle":"2024-12-04T05:16:16.541349Z","shell.execute_reply.started":"2024-12-04T05:16:16.534963Z","shell.execute_reply":"2024-12-04T05:16:16.540617Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# read test and benchmark data\ntest_dataset = prepare_data(\"./test_split/test.eng_Latn\", \"./test_split/test.mai_Deva\", \"test\")\n\n# choose random 1200 examples from test dataset for faster inference and evaluation\ntest_dataset = test_dataset.shuffle(seed=42).select(range(1200))\ntest_dataset_tokenized = tokenize_dataset(test_dataset, tokenizer)\n\n# generate predictions for english to hindi\neval_trainer = trainer_evaluate(model, tokenizer, test_dataset_tokenized)\ntest_dataset_mai_pred, test_dataset_mai_lab, _ = eval_trainer.predict(test_dataset_tokenized)\n\n# decode the predictions and references\ntest_dataset_mai_pred = tokenizer.batch_decode(test_dataset_mai_pred, skip_special_tokens=True)\ntest_dataset_mai_ref = tokenizer.batch_decode(test_dataset_mai_lab, skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T05:16:19.708115Z","iopub.execute_input":"2024-12-04T05:16:19.708955Z","iopub.status.idle":"2024-12-04T05:18:47.039303Z","shell.execute_reply.started":"2024-12-04T05:16:19.708921Z","shell.execute_reply":"2024-12-04T05:18:47.038577Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb0ab8a821f84d69944752a2029fbf3f"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_23/2343488411.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Seq2SeqTrainer(\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# print the predictions and references for comparison\nprint(\"Testing results for test dataset\\n\")\nprint(\"Test dataset size:\", len(test_dataset))\nfor i in range(2):\n    print(\"English Text : \", test_dataset[i]['source_text'])\n    print(\"Maithili Reference (Original) : \", test_dataset[i]['target_text'])\n    print(\"Maithili Reference (Decoded): \", test_dataset_mai_ref[i])\n    print(\"Maithili Prediction: \",test_dataset_mai_pred[i])\n    print(\"\\n\")\n\n# calculate chrF++ score for hindi to maithili overlap\nchrf_score_mai = compute_chrf(test_dataset_mai_pred, test_dataset_mai_ref)\nprint(f\"chrF++ score for English-Maithili test data split: {chrf_score_mai['score']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T05:22:16.786454Z","iopub.execute_input":"2024-12-04T05:22:16.787086Z","iopub.status.idle":"2024-12-04T05:22:17.486629Z","shell.execute_reply.started":"2024-12-04T05:22:16.787051Z","shell.execute_reply":"2024-12-04T05:22:17.485844Z"}},"outputs":[{"name":"stdout","text":"Testing results for test dataset\n\nTest dataset size: 1200\nEnglish Text :  The car makes Kirby faster, while the dolphin allows him to go underwater.\nMaithili Reference (Original) :  कार किर्बीकेँ तेज बनबैत अछि, जखनकि डॉल्फिन ओकरा पानिक नीचाँ जाय दैत अछि।\nMaithili Reference (Decoded):  कार किर्बी तेज बनब , नकि डॉल्फिन रा पानिक नीचाँ जाय\nMaithili Prediction:  कार किरबी तेजी बनब , नकि फिल्फिन हुनका पातमे जाइत\n\n\nEnglish Text :  Both teams will have Friday off before the Penguins battle the Rangers and the Devils face the Sabres on Saturday.\nMaithili Reference (Original) :  दुनू टीमकेँ शुक्रदिन छुट्टी भेटतनि जेकरा बाद शनिदिन पहिने पेंगुइनक मैच रेंजर्ससँ आ डेविल्सक मैच सबरेजसँ होयत।\nMaithili Reference (Decoded):  दुनू टीम ्रदिन ट्टी टतनि रा बाद शनन पने पेंगुइनक म रेंजर् आ ल्सक म सबर हो\nMaithili Prediction:  दुनू टीम पेंगुइन ्डर्स ्ध  पने फेंगुइन   आ सॉफ्ट शन् सबर\n\n\nchrF++ score for English-Maithili test data split: 34.759289318711595\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"### Part 4) Testing finetuned model's performance on benchmark datasets","metadata":{}},{"cell_type":"code","source":"# prepare and tokenize the in22 benchmark datasets\nin22_mai_test = prepare_data(\"/kaggle/input/in22-test-data/IN22_test/gen/test.eng_Latn\", \"/kaggle/input/in22-test-data/IN22_test/gen/test.mai_Deva\", \"test\")\nin22_mai_test_tokenized = tokenize_dataset(in22_mai_test, tokenizer)\n\n# generate predictions for english to maithili\neval_trainer = trainer_evaluate(model, tokenizer, in22_mai_test_tokenized)\nin22_mai_test_pred, in22_mai_test_lab, _ = eval_trainer.predict(in22_mai_test_tokenized)\n\n# decode the predictions and references\nin22_mai_test_pred = tokenizer.batch_decode(in22_mai_test_pred, skip_special_tokens=True)\nin22_mai_test_ref = tokenizer.batch_decode(in22_mai_test_lab, skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T05:23:16.350813Z","iopub.execute_input":"2024-12-04T05:23:16.351151Z","iopub.status.idle":"2024-12-04T05:25:37.225511Z","shell.execute_reply.started":"2024-12-04T05:23:16.351120Z","shell.execute_reply":"2024-12-04T05:25:37.224534Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1024 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eacd1179fa6d41fbbc4f146a90094aa0"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_23/2343488411.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  eval_trainer = Seq2SeqTrainer(\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"print(\"Testing results for IN22 benchmark dataset\\n\")\nprint(\"IN22 benchmark dataset size:\", len(in22_mai_test))\n\n# print the predictions and references for comparison\nfor i in range(2):\n    print(\"English Text: \", in22_mai_test[i][\"source_text\"])\n    print(\"Maithili Reference (Original) : \", in22_mai_test[i]['target_text'])\n    print(\"Maithili Reference (Decoded): \", in22_mai_test_ref[i])\n    print(\"Maithili Prediction: \", in22_mai_test_pred[i])\n    print(\"\\n\")\n\n# calculate chrF++ score for hindi to maithili overlap\nchrf_score_mai = compute_chrf(in22_mai_test_pred, in22_mai_test_ref)\nprint(f\"chrF++ score for English-Maithili IN22 benchmark dataset: {chrf_score_mai['score']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T05:25:54.750327Z","iopub.execute_input":"2024-12-04T05:25:54.750642Z","iopub.status.idle":"2024-12-04T05:25:55.504269Z","shell.execute_reply.started":"2024-12-04T05:25:54.750613Z","shell.execute_reply":"2024-12-04T05:25:55.503389Z"}},"outputs":[{"name":"stdout","text":"Testing results for IN22 benchmark dataset\n\nIN22 benchmark dataset size: 1024\nEnglish Text:  An appearance is a bunch of attributes related to the service person, like their shoes, clothes, tie, jewellery, hairstyle, make-up, watch, cosmetics, perfume, etc.\nMaithili Reference (Original) :  रूप सर्विसवला व्यक्तिसँ सम्बन्धित बहुत रास लक्षणक समूह होयत छै जेना हुनक जूता, कपड़ा, टाई, गहना, केश, श्रृंगार, घड़ी, प्रसाधन सामग्री, सेंट इत्यादि।\nMaithili Reference (Decoded):  रूप सर्ला व्यक् सम्बन् ब रास लक् सम होयत  जेना हुनक ा, कपड़ा, टाई, गहना, , श्रृंगार, ़ी, प्रसाधन सामग्री, सेंट इत्या\nMaithili Prediction:  एकटा उपकरण सेवा व्यक्ति सम्बन् एक टा  , जेना हुनकर पाथ, कपड़, ार, गारी, ्पाली, बन्द, ्सा, ्सा, ्सा, आ सम्बन् सम्मिलित\n\n\nEnglish Text:  Ajanta, located in the Aurangabad District of Maharashtra has twenty-nine caitya and vihara caves decorated with sculptures and paintings from the first century B.C.E. to the fifth century C.E.\nMaithili Reference (Original) :  महाराष्ट्रके औरंगाबादमे स्थित अजन्तामे पहिल शताब्दी ईसा पूर्व सँ पाँचम शताब्दी धरिक मूर्तिकला आ चित्रकला सँ सजाओल उन्नतीस टा चैत्य आ विहार अछि।\nMaithili Reference (Decoded):  महाराष्ट्रके औरंगाबादमे स्थित न्तामे पल ाब्दी ा पूर्व  पाम ाब्दी धरिक मूर्तिकला आ चित्रकला  ाओल उन्नतीस टा ्य आ ार\nMaithili Prediction:  महाराष्ट्रक अरङ्गाबाद जिलामे स्थित ान्ता पल ी ाब्दीक मूर्ति आ भारा गुफा ्ग पल ा पाम ाब्दीमे ल छल।\n\n\nchrF++ score for English-Maithili IN22 benchmark dataset: 28.217431385507236\n","output_type":"stream"}],"execution_count":23}]}